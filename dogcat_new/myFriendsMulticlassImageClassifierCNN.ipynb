{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\Kishan\\Anaconda3\\envs\\dogcat_test\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\Kishan\\Anaconda3\\envs\\dogcat_test\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\Kishan\\Anaconda3\\envs\\dogcat_test\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\Kishan\\Anaconda3\\envs\\dogcat_test\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\Kishan\\Anaconda3\\envs\\dogcat_test\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\Kishan\\Anaconda3\\envs\\dogcat_test\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "#importing the librariers to create the model\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a pipleline sequential\n",
    "\n",
    "image_classifier = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a convolution layer and setting the kernels\n",
    "\n",
    "image_classifier.add(Convolution2D(32,(3,3),input_shape=(64,64,3),activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Kishan\\Anaconda3\\envs\\dogcat_test\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#creating a pooling layers\n",
    "image_classifier.add(MaxPooling2D(pool_size=(3,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Flattening the convoluted data\n",
    "\n",
    "image_classifier.add(Flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding neural network\n",
    "\n",
    "image_classifier.add(Dense(units=64,activation='softmax'))\n",
    "image_classifier.add(Dense(units=3,activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compling the CNN\n",
    "image_classifier.compile(optimizer='Adam',loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "#tf.keras.losses.categorical_crossentropy\n",
    "#tf.keras.optimizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitting tghe CNN to images\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen= ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.keras.losses.categorical_crossentropy\n",
    "#tf.keras.optimizers. This is how you can check the loss and optimizer and activation function available iwth name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 33 images belonging to 3 classes.\n",
      "Found 33 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "#creating the dataset\n",
    "training_set= train_datagen.flow_from_directory(r'F:\\images_CNN',target_size=(64,64),batch_size=32,class_mode='categorical')\n",
    "test_set= test_datagen.flow_from_directory(r'F:\\images_CNN',target_size=(64,64),batch_size=32,class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kishan\\Anaconda3\\envs\\dogcat_test\\lib\\site-packages\\keras\\utils\\data_utils.py:616: UserWarning: The input 0 could not be retrieved. It could be because a worker has died.\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "262/800 [========>.....................] - ETA: 1:59:02 - loss: 0.0592 - accuracy: 1.0000"
     ]
    }
   ],
   "source": [
    "image_classify_model= image_classifier.fit_generator(training_set,steps_per_epoch=800,epochs=2,validation_data=test_set,validation_steps=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "image_classifier.save(\"image_classify_model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Baby': 0, 'Kishan': 1, 'Rakesh': 2}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'epochs': 2,\n",
       " 'steps': 800,\n",
       " 'verbose': 1,\n",
       " 'do_validation': True,\n",
       " 'metrics': ['loss', 'accuracy', 'val_loss', 'val_accuracy']}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_classify_model.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "its Rakesh\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing import image\n",
    "test_image= image.load_img(r'F:\\Gujrat trop\\IMG_20190219_112645.jpg',target_size=(64,64))\n",
    "test_image= image.img_to_array(test_image)\n",
    "test_image=np.expand_dims(test_image,axis=0)\n",
    "model=load_model(\"image_classify_model.h5\")\n",
    "result=model.predict_proba(test_image)\n",
    "if np.argmax(result[0]) == 0:\n",
    "    print(\"its baby\")\n",
    "elif np.argmax(result[0]) == 1:\n",
    "    print(\"Its Kishan\")\n",
    "else:\n",
    "    print(\"its Rakesh\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.03380242, 0.02803168, 0.93816584]], dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "nn=np.load(r'C:/Users/Kishan/Downloads/conv2d_1_kernel_0 (1).npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.12050168,  0.03848144, -0.11384341, -0.08852763, -0.00787206,\n",
       "         0.15771195, -0.01851995, -0.09459576,  0.09650394,  0.12857354,\n",
       "        -0.02361998, -0.07557377, -0.01236992,  0.025731  , -0.10494816,\n",
       "        -0.05806601, -0.050201  , -0.10117855, -0.13493995,  0.06230396,\n",
       "        -0.05232795, -0.02253981,  0.04285239,  0.10415566, -0.18098663,\n",
       "         0.06549856, -0.05211704, -0.10665832, -0.1207348 , -0.09216708,\n",
       "         0.10079101, -0.02211122],\n",
       "       [ 0.0377595 , -0.09808479,  0.06306689, -0.036848  ,  0.01040586,\n",
       "         0.10216539, -0.08481596,  0.13378517,  0.03966426,  0.0148929 ,\n",
       "        -0.12681775,  0.11465637, -0.06648129, -0.04459801,  0.11473614,\n",
       "         0.01414053,  0.05510355, -0.12543698, -0.1297576 , -0.07653151,\n",
       "        -0.03219608, -0.07224974, -0.08421243,  0.01752902, -0.15432309,\n",
       "        -0.0294784 ,  0.03504584,  0.1031573 ,  0.09856116,  0.12516576,\n",
       "        -0.13113822, -0.09152312],\n",
       "       [-0.02174964, -0.10228779, -0.10854736,  0.12584217, -0.11603716,\n",
       "        -0.07188134,  0.09785535, -0.06266864,  0.089081  , -0.02683245,\n",
       "         0.10259806, -0.02749385,  0.13289303, -0.02601698, -0.10573851,\n",
       "        -0.03635626, -0.04999159, -0.03542865,  0.1292808 ,  0.08803789,\n",
       "        -0.0482326 ,  0.02555752,  0.09119938, -0.00568882,  0.0078059 ,\n",
       "         0.0251876 , -0.0673709 , -0.12251863,  0.16082524,  0.13685247,\n",
       "        -0.00193102,  0.00656079]], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "for n in range(2):\n",
    "    plt.imshow(nn[n][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
